{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sublayers\n",
    "Note the usage of Conv1d here for multiple attention heads. This is supposed to replicate the linear transformation represented for each head in one structure, but make sure it's correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, heads=1):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.w_query = nn.Conv1d(embedding_dim, heads*embedding_dim, 1)\n",
    "        self.w_key = nn.Conv1d(embedding_dim, heads*embedding_dim, 1)\n",
    "        self.w_value = nn.Conv1d(embedding_dim, heads*embedding_dim, 1)\n",
    "        nn.init.normal_(self.w_query.weight, mean=0, std=np.sqrt(2.0 / (embedding_dim + (heads*embedding_dim))))\n",
    "        nn.init.normal_(self.w_key.weight, mean=0, std=np.sqrt(2.0 / (embedding_dim + (heads*embedding_dim))))\n",
    "        nn.init.normal_(self.w_value.weight, mean=0, std=np.sqrt(2.0 / (embedding_dim + (heads*embedding_dim))))\n",
    "        self.scale_factor = 1. / np.sqrt(embedding_dim)\n",
    "        self.w_out = nn.Linear(heads*embedding_dim, embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, input, mask=None):\n",
    "        if mask is None:\n",
    "            masked_input = input\n",
    "        else:\n",
    "            masked_input = mask * input\n",
    "        q = self.w_query(masked_input.transpose(1, 2)) \n",
    "        k = self.w_key(masked_input.transpose(1, 2)) \n",
    "        v = self.w_value(masked_input.transpose(1, 2))\n",
    "        print(torch.bmm(q, k.transpose(1, 2)).shape)\n",
    "        attention_weights = F.softmax(torch.bmm(q, k.transpose(1, 2)), dim=-1) / self.scale_factor\n",
    "        output = torch.bmm(attention_weights, v).transpose(1, 2)\n",
    "        output = self.w_out(output)\n",
    "        output = self.layer_norm(input + output)\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class PositionwiseFeedForwardLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(PositionwiseFeedForwardLayer, self).__init__()\n",
    "        self.w_1 = nn.Conv1d(embedding_dim, hidden_dim, 1) \n",
    "        self.w_2 = nn.Conv1d(hidden_dim, embedding_dim, 1) \n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.w_2(F.relu(self.w_1(input.transpose(1, 2))))\n",
    "        output = self.layer_norm(input + output.transpose(1, 2))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-level Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Taken from http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding'''\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()        \n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(seq_len, embedding_dim)\n",
    "        position = torch.arange(0., seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., embedding_dim, 2) *\n",
    "                             -(math.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return x\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embedding_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(seq_len, embedding_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.pos_encoding(self.embedding(input) * np.sqrt(embedding_dim))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, heads=1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = AttentionLayer(embedding_dim, heads=heads)\n",
    "        self.pwff = PositionwiseFeedForwardLayer(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, input, detailed=False):\n",
    "        output, attention_weights = self.attention(input)\n",
    "        output = self.pwff(output)\n",
    "        if detailed:\n",
    "            return output, attention_weights\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, heads=1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.attention = AttentionLayer(embedding_dim, heads=heads)\n",
    "        self.pwff = PositionwiseFeedForwardLayer(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, input, detailed=False):\n",
    "        output, attention_weights = self.attention(input)\n",
    "        output = self.pwff(output)\n",
    "        if detailed:\n",
    "            return output, attention_weights\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = torch.LongTensor([[0, 1, 2], [3, 4, 5]])\n",
    "label = torch.LongTensor([[2, 1, 0, 6], [5, 4, 3, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate LayerNorm and residual connections, all vector dimensions will be the same (embedding dimension and output dimensions, i.e. anything that needs to be added together, hence why ```EncoderLayer``` dimensions don't require a hidden dimension for the ```AttentionLayer``` (all dimensions are 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "seq_len = 3\n",
    "embedding_dim = 10\n",
    "embedding_layer = EmbeddingLayer(vocab_size, vocab_size, embedding_dim)\n",
    "encoder_layer = EncoderLayer(embedding_dim, 5, heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1137, -0.3880, -1.5422, -0.4232,  0.6086,  1.4954, -1.0851,\n",
       "          -1.1498,  0.7424,  0.6282],\n",
       "         [ 1.5668, -0.4738, -0.0204, -0.1533, -0.0912,  1.6796, -0.7200,\n",
       "          -1.9743,  0.2231, -0.0365],\n",
       "         [-0.7582,  0.1845,  0.8691, -1.0817,  1.0172,  0.2928, -1.9719,\n",
       "          -0.5819,  0.8459,  1.1843]],\n",
       "\n",
       "        [[ 0.6240, -1.1185, -0.9015,  0.9725, -0.9600,  1.7241, -1.1003,\n",
       "          -0.4100,  0.0177,  1.1522],\n",
       "         [ 1.7745, -0.4261, -0.8353,  1.4753, -0.3191,  0.1992, -1.5761,\n",
       "          -0.9651,  0.2581,  0.4144],\n",
       "         [-0.8022,  0.9486,  0.3475, -0.2479, -0.1137,  2.1452, -0.9754,\n",
       "          -0.5459, -1.4086,  0.6525]]], grad_fn=<AddcmulBackward>)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer(embedding_layer(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "How to deal with masking, how to get sequential outputs from decoder, how to feed inputs to decoder (like the memory from encoder, the source mask, the target mask, etc.)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
